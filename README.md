ğŸ¥ Healthcare ETL Pipeline with PySpark, AWS, Airflow & Docker

ğŸ“Œ Project Overview
This project implements a production-grade Healthcare ETL pipeline using PySpark, AWS S3, Airflow, and Docker.
It simulates real-world healthcare data, processes it through Bronze â†’ Silver â†’ Gold layers, and produces analytics-ready datasets and KPIs using a modular, scalable architecture.
The pipeline is fully containerized using Docker and orchestrated with Apache Airflow, making it deployment-ready and cloud-friendly.

ğŸ§° Tech Stack & Tools
Programming Language: Python
Big Data Processing: PySpark
Workflow Orchestration: Apache Airflow
Cloud Storage: AWS S3
Containerization: Docker & Docker Compose
Data Generation: Faker
Configuration Management: JSON, .env
Storage Format: Parquet
ğŸ—ï¸ High-Level Architecture
Airflow DAG
    |
    â–¼
Main Orchestration Script
    |
    â–¼
Spark Session Initialization
    |
    â–¼
Synthetic Data Generation (Faker)
    |
    â–¼
Bronze Layer (Raw Ingestion)
    |
    â–¼
Data Validation
    |
    â–¼
Silver Layer (Cleaned & Standardized)
    |
    â–¼
Gold Layer (Facts & Dimensions)
    |
    â–¼
Analytics & KPIs
    |
    â–¼
Write to S3 / Local Storage
ğŸ“‚ Project Structure
healthcare-etl-pyspark/
â”‚
â”œâ”€â”€ dags/                         # Airflow DAGs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ spark_jobs/
â”‚   â”‚   â”œâ”€â”€ bronze_level/         # Data ingestion
â”‚   â”‚   â”œâ”€â”€ silver_level/         # Data cleaning
â”‚   â”‚   â””â”€â”€ gold_level/           # Analytics & KPIs
â”‚   â””â”€â”€ utils/                    # Spark session, helpers
â”‚
â”œâ”€â”€ main_pipeline_orchestration.py
â”œâ”€â”€ health_careEtl.json           # Config file
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md

ğŸ”„ ETL Pipeline Layers
ğŸ¥‰ Bronze Layer â€“ Raw Ingestion
Ingests raw CSV healthcare data:
Patients
Encounters
Treatments
Adds metadata:
Ingestion timestamp
Batch ID
Source file name
ğŸ¥ˆ Silver Layer â€“ Data Cleaning & Validation
Removes duplicates
Handles null values
Enforces foreign-key relationships
Standardizes categorical fields
Adds derived columns (e.g., billable flag)

ğŸ¥‡ Gold Layer â€“ Analytics & KPIs
Dimension Tables
dim_patient
dim_doctor
dim_hospital_unit
dim_date
Fact Tables
fact_encounters
fact_treatments

ğŸ“Š Analytics & Insights Extracted
From the Gold layer, the pipeline generates:
Patient demographics & registration trends
Hospital visit analysis
Encounters per department
Encounter type distribution
Treatment analytics
Billable vs non-billable treatments
Treatment cost aggregation
Operational KPIs
Encounter cancellation rate
Revenue loss due to cancellations
Total treatment revenue
Healthcare operational insights
Department-level workload analysis
Financial performance indicators

â±ï¸ Orchestration with Airflow
Airflow DAG schedules and monitors the ETL pipeline
Handles:
Job retries
Failure alerts
Dependency management
Enables fully automated batch processing

ğŸ³ Dockerized Deployment
Docker ensures:
Consistent runtime environment
Easy local & cloud deployment
Docker Compose spins up:
Airflow services
Spark environment
Supporting infrastructure

â–¶ï¸ How to Run the Project
1ï¸âƒ£ Clone Repository
git clone https://github.com/your-username/healthcare-etl-pyspark.git
cd healthcare-etl-pyspark
2ï¸âƒ£ Configure Environment
Update .env with:
AWS credentials
Environment variables
3ï¸âƒ£ Start Services
docker-compose up -d
4ï¸âƒ£ Trigger Pipeline
Open Airflow UI
Enable DAG
Trigger ETL run
ğŸš€ Key Highlights
Production-grade modular architecture
Clear Bronze / Silver / Gold layering
Airflow-driven orchestration
Dockerized & cloud-ready
Realistic healthcare analytics use cases
